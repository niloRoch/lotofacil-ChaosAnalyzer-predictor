# -*- coding: utf-8 -*-
"""CHaos_otimizadoLF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12BIddeVlPpypCcOiAHbNwiOMZV6EjS4Z
"""

# -*- coding: utf-8 -*-
"""
PREDITOR LOTOFÁCIL APRIMORADO - VERSÃO 2.0
Melhorias focadas em assertividade:
- Análise estatística mais rigorosa
- Ensemble com validação cruzada
- Filtros adaptativos
- Otimização de hiperparâmetros
"""

import pandas as pd
import numpy as np
import warnings
from collections import defaultdict, Counter
from typing import List, Tuple, Dict, Any, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import xgboost as xgb
from scipy import stats
from scipy.optimize import minimize
from scipy.signal import find_peaks, savgol_filter
import itertools
from tqdm import tqdm
from google import colab as cl
warnings.filterwarnings('ignore')

#importando arq
file_upload = cl.files.upload()

class AdvancedStatisticalAnalyzer:
    """Analisador estatístico avançado com foco em padrões reais"""

    def __init__(self):
        self.frequency_analysis = {}
        self.position_analysis = {}
        self.pattern_cache = {}

    def comprehensive_frequency_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Análise de frequência abrangente"""
        analysis = {}

        # Frequência global de cada número
        global_freq = Counter()
        position_freq = {i: Counter() for i in range(1, 16)}

        for _, row in df.iterrows():
            numbers = [row[f'Bola{i}'] for i in range(1, 16)]
            for pos, num in enumerate(numbers, 1):
                global_freq[num] += 1
                position_freq[pos][num] += 1

        analysis['global_frequency'] = dict(global_freq)
        analysis['position_frequency'] = {pos: dict(freq) for pos, freq in position_freq.items()}

        # Análise de tendências temporais (últimos N concursos)
        for window in [10, 20, 50, 100]:
            if len(df) >= window:
                recent_df = df.tail(window)
                recent_freq = Counter()
                for _, row in recent_df.iterrows():
                    numbers = [row[f'Bola{i}'] for i in range(1, 16)]
                    for num in numbers:
                        recent_freq[num] += 1

                analysis[f'frequency_last_{window}'] = dict(recent_freq)

        # Análise de periodicidade
        analysis['periodicity'] = self._analyze_periodicity(df)

        # Análise de gaps (intervalos entre aparições)
        analysis['gap_analysis'] = self._analyze_gaps(df)

        return analysis

    def _analyze_periodicity(self, df: pd.DataFrame) -> Dict[int, Dict[str, float]]:
        """Analisar periodicidade de cada número"""
        periodicity = {}

        for num in range(1, 26):
            appearances = []
            for i, row in df.iterrows():
                numbers = [row[f'Bola{j}'] for j in range(1, 16)]
                if num in numbers:
                    appearances.append(i)

            if len(appearances) > 2:
                intervals = np.diff(appearances)
                periodicity[num] = {
                    'mean_interval': np.mean(intervals),
                    'std_interval': np.std(intervals),
                    'median_interval': np.median(intervals),
                    'last_appearance': len(df) - 1 - appearances[-1] if appearances else len(df)
                }
            else:
                periodicity[num] = {
                    'mean_interval': len(df),
                    'std_interval': 0,
                    'median_interval': len(df),
                    'last_appearance': len(df)
                }

        return periodicity

    def _analyze_gaps(self, df: pd.DataFrame) -> Dict[int, List[int]]:
        """Analisar gaps entre aparições"""
        gaps = {num: [] for num in range(1, 26)}
        last_seen = {num: -1 for num in range(1, 26)}

        for i, row in df.iterrows():
            numbers = [row[f'Bola{j}'] for j in range(1, 16)]

            # Calcular gaps para números que apareceram
            for num in numbers:
                if last_seen[num] != -1:
                    gap = i - last_seen[num]
                    gaps[num].append(gap)
                last_seen[num] = i

        return gaps

    def predict_by_statistical_model(self, df: pd.DataFrame, n_predictions: int = 18) -> List[int]:
        """Predição baseada em modelo estatístico avançado"""
        analysis = self.comprehensive_frequency_analysis(df)

        # Calcular scores para cada número
        scores = {}
        total_draws = len(df)

        for num in range(1, 26):
            score = 0

            # 1. Score baseado em frequência global normalizada
            global_freq = analysis['global_frequency'].get(num, 0)
            expected_freq = total_draws * 15 / 25  # Frequência esperada
            freq_score = (expected_freq - global_freq) / expected_freq  # Favorece números "atrasados"

            # 2. Score baseado em tendência recente
            recent_trend = 0
            for window in [10, 20, 50]:
                if f'frequency_last_{window}' in analysis:
                    recent_freq = analysis[f'frequency_last_{window}'].get(num, 0)
                    expected_recent = window * 15 / 25
                    trend_component = (expected_recent - recent_freq) / expected_recent
                    recent_trend += trend_component * (1 / window)  # Peso maior para janelas menores

            # 3. Score baseado em periodicidade
            periodicity = analysis['periodicity'][num]
            last_gap = periodicity['last_appearance']
            mean_interval = periodicity['mean_interval']

            if mean_interval > 0:
                periodicity_score = min(2.0, last_gap / mean_interval)
            else:
                periodicity_score = 1.0

            # 4. Score baseado em distribuição de gaps
            gaps = analysis['gap_analysis'][num]
            if gaps:
                gap_mean = np.mean(gaps)
                gap_std = np.std(gaps) if len(gaps) > 1 else gap_mean

                # Probabilidade baseada na distribuição normal dos gaps
                if gap_std > 0:
                    z_score = (last_gap - gap_mean) / gap_std
                    gap_probability = stats.norm.cdf(z_score)
                else:
                    gap_probability = 0.5
            else:
                gap_probability = 1.0

            # Combinar scores com pesos otimizados
            score = (
                freq_score * 0.3 +
                recent_trend * 0.25 +
                periodicity_score * 0.25 +
                gap_probability * 0.2
            )

            scores[num] = score

        # Selecionar números com maiores scores
        sorted_numbers = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers[:n_predictions]]

class SmartFeatureEngineering:
    """Engenharia de features inteligente"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_selector = None

    def create_advanced_features(self, df: pd.DataFrame) -> np.ndarray:
        """Criar features avançadas para ML"""
        features = []

        # Para cada concurso, criar features
        for i in range(len(df)):
            row_features = []

            # 1. Features básicas do sorteio atual
            numbers = [df.iloc[i][f'Bola{j}'] for j in range(1, 16)]

            # Estatísticas descritivas
            row_features.extend([
                np.mean(numbers),
                np.std(numbers),
                np.median(numbers),
                np.min(numbers),
                np.max(numbers),
                np.sum(numbers)
            ])

            # 2. Features de distribuição
            # Números por faixa
            baixa = sum(1 for n in numbers if n <= 8)
            media = sum(1 for n in numbers if 9 <= n <= 17)
            alta = sum(1 for n in numbers if n >= 18)

            row_features.extend([baixa, media, alta])

            # Paridade
            pares = sum(1 for n in numbers if n % 2 == 0)
            impares = 15 - pares
            row_features.extend([pares, impares])

            # 3. Features temporais (lookback)
            lookback_features = []
            for lookback in [1, 2, 3, 5, 10]:
                if i >= lookback:
                    # Números dos sorteios anteriores
                    prev_numbers = []
                    for j in range(max(0, i-lookback), i):
                        prev_nums = [df.iloc[j][f'Bola{k}'] for k in range(1, 16)]
                        prev_numbers.extend(prev_nums)

                    if prev_numbers:
                        lookback_features.extend([
                            np.mean(prev_numbers),
                            np.std(prev_numbers),
                            len(set(prev_numbers))  # Números únicos
                        ])
                    else:
                        lookback_features.extend([0, 0, 0])
                else:
                    lookback_features.extend([0, 0, 0])

            row_features.extend(lookback_features)

            # 4. Features de padrões
            # Sequências consecutivas
            sorted_numbers = sorted(numbers)
            consecutive_count = 0
            for j in range(len(sorted_numbers) - 1):
                if sorted_numbers[j+1] - sorted_numbers[j] == 1:
                    consecutive_count += 1
            row_features.append(consecutive_count)

            # Gaps entre números
            gaps = [sorted_numbers[j+1] - sorted_numbers[j] for j in range(len(sorted_numbers) - 1)]
            if gaps:
                row_features.extend([np.mean(gaps), np.std(gaps)])
            else:
                row_features.extend([0, 0])

            # 5. Features de frequência histórica
            if i > 0:
                # Frequência dos números no histórico até este ponto
                historical_df = df.iloc[:i]
                freq_features = []

                for num in numbers:
                    count = 0
                    for _, hist_row in historical_df.iterrows():
                        hist_numbers = [hist_row[f'Bola{k}'] for k in range(1, 16)]
                        if num in hist_numbers:
                            count += 1

                    freq_features.append(count / len(historical_df) if len(historical_df) > 0 else 0)

                # Estatísticas das frequências
                if freq_features:
                    row_features.extend([
                        np.mean(freq_features),
                        np.std(freq_features),
                        np.min(freq_features),
                        np.max(freq_features)
                    ])
                else:
                    row_features.extend([0, 0, 0, 0])
            else:
                row_features.extend([0, 0, 0, 0])

            features.append(row_features)

        return np.array(features)

    def select_best_features(self, X: np.ndarray, y: np.ndarray, k: int = 50) -> np.ndarray:
        """Selecionar as melhores features"""
        if self.feature_selector is None:
            self.feature_selector = SelectKBest(score_func=mutual_info_regression, k=min(k, X.shape[1]))
            X_selected = self.feature_selector.fit_transform(X, y.flatten())
        else:
            X_selected = self.feature_selector.transform(X)

        return X_selected

class EnsemblePredictor:
    """Ensemble de múltiplos algoritmos otimizado"""

    def __init__(self):
        self.models = {}
        self.weights = {}
        self.scaler = RobustScaler()
        self.feature_engineer = SmartFeatureEngineering()

    def create_models(self) -> Dict[str, Any]:
        """Criar ensemble de modelos otimizados"""
        models = {}

        # Random Forest otimizado
        models['rf'] = RandomForestRegressor(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )

        # XGBoost otimizado
        models['xgb'] = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )

        # Gradient Boosting
        models['gb'] = GradientBoostingRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42
        )

        # Neural Network
        models['mlp'] = MLPRegressor(
            hidden_layer_sizes=(100, 50, 25),
            activation='relu',
            solver='adam',
            alpha=0.001,
            learning_rate='adaptive',
            max_iter=500,
            random_state=42
        )

        return models

    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Preparar dados para treinamento"""
        # Criar features
        X = self.feature_engineer.create_advanced_features(df)

        # Criar targets (próximo sorteio)
        y = []
        for i in range(len(df) - 1):
            next_numbers = [df.iloc[i+1][f'Bola{j}'] for j in range(1, 16)]
            y.append(next_numbers)

        # Ajustar X para corresponder a y
        X = X[:-1]  # Remover última linha que não tem target
        y = np.array(y)

        # Selecionar melhores features
        X_selected = self.feature_engineer.select_best_features(X, y)

        # Normalizar
        X_normalized = self.scaler.fit_transform(X_selected)

        return X_normalized, y

    def train_with_cross_validation(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Treinar com validação cruzada temporal"""
        X, y = self.prepare_training_data(df)

        if len(X) < 10:
            raise ValueError("Dados insuficientes para treinamento")

        models = self.create_models()
        results = {}

        # Validação cruzada temporal
        tscv = TimeSeriesSplit(n_splits=5)

        print("Treinando ensemble com validação cruzada...")

        for name, model in tqdm(models.items()):
            scores = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                # Treinar modelo para cada posição
                position_scores = []
                for pos in range(15):
                    model_copy = type(model)(**model.get_params())
                    model_copy.fit(X_train, y_train[:, pos])

                    y_pred = model_copy.predict(X_val)
                    score = -mean_absolute_error(y_val[:, pos], y_pred)
                    position_scores.append(score)

                scores.append(np.mean(position_scores))

            avg_score = np.mean(scores)
            results[name] = {
                'model': model,
                'cv_score': avg_score,
                'cv_std': np.std(scores)
            }

            print(f"{name}: CV Score = {avg_score:.4f} (+/- {np.std(scores):.4f})")

        # Treinar modelos finais
        self.models = {}
        for name, result in results.items():
            model = result['model']
            # Treinar um modelo para cada posição
            position_models = []
            for pos in range(15):
                model_copy = type(model)(**model.get_params())
                model_copy.fit(X, y[:, pos])
                position_models.append(model_copy)
            self.models[name] = position_models

        # Calcular pesos baseados na performance
        scores = [results[name]['cv_score'] for name in results.keys()]
        min_score = min(scores)
        shifted_scores = [score - min_score + 1e-6 for score in scores]
        total_score = sum(shifted_scores)

        self.weights = {
            name: shifted_scores[i] / total_score
            for i, name in enumerate(results.keys())
        }

        print(f"\nPesos finais: {self.weights}")

        return results

    def predict(self, df: pd.DataFrame, n_predictions: int = 18) -> List[int]:
        """Fazer predição usando ensemble"""
        if not self.models:
            raise ValueError("Modelos não foram treinados")

        # Preparar features para predição
        X = self.feature_engineer.create_advanced_features(df)
        X_selected = self.feature_engineer.feature_selector.transform(X[-1:])
        X_normalized = self.scaler.transform(X_selected)

        # Predições de cada modelo
        ensemble_predictions = []

        for name, position_models in self.models.items():
            model_pred = []
            for pos in range(15):
                pred = position_models[pos].predict(X_normalized)[0]
                model_pred.append(max(1, min(25, round(pred))))

            ensemble_predictions.append(model_pred)

        # Combinar predições com pesos
        final_scores = {num: 0 for num in range(1, 26)}

        for i, predictions in enumerate(ensemble_predictions):
            model_name = list(self.models.keys())[i]
            weight = self.weights[model_name]

            for pred in predictions:
                final_scores[pred] += weight

        # Adicionar ruído pequeno para quebrar empates
        for num in final_scores:
            final_scores[num] += np.random.normal(0, 0.01)

        # Selecionar top números
        sorted_numbers = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers[:n_predictions]]

class AdaptiveFilterSystem:
    """Sistema de filtros adaptativos"""

    def __init__(self):
        self.filters = []

    def apply_statistical_filters(self, numbers: List[int], df: pd.DataFrame) -> List[int]:
        """Aplicar filtros estatísticos"""
        filtered = numbers.copy()

        # Filtro 1: Balanceamento par/ímpar
        pares = [n for n in filtered if n % 2 == 0]
        impares = [n for n in filtered if n % 2 == 1]

        # Análise histórica de paridade
        historical_ratios = []
        for _, row in df.tail(50).iterrows():
            row_numbers = [row[f'Bola{i}'] for i in range(1, 16)]
            pares_hist = sum(1 for n in row_numbers if n % 2 == 0)
            historical_ratios.append(pares_hist / 15)

        target_par_ratio = np.mean(historical_ratios)
        target_pares = int(len(filtered) * target_par_ratio)

        # Ajustar para atingir ratio histórico
        if len(pares) > target_pares:
            # Remover alguns pares
            excess_pares = pares[target_pares:]
            filtered = [n for n in filtered if n not in excess_pares]
        elif len(pares) < target_pares:
            # Adicionar pares se possível
            needed_pares = target_pares - len(pares)
            available_pares = [n for n in range(2, 26, 2) if n not in filtered]
            if available_pares:
                to_add = available_pares[:needed_pares]
                filtered.extend(to_add)

        # Filtro 2: Distribuição por faixas
        baixa = [n for n in filtered if n <= 8]
        media = [n for n in filtered if 9 <= n <= 17]
        alta = [n for n in filtered if n >= 18]

        # Balancear faixas baseado no histórico
        ideal_baixa = len(filtered) // 3
        ideal_media = len(filtered) // 3
        ideal_alta = len(filtered) - ideal_baixa - ideal_media

        # Ajustes simples
        if len(baixa) > ideal_baixa + 2:
            excess = baixa[ideal_baixa + 1:]
            filtered = [n for n in filtered if n not in excess]

        if len(alta) > ideal_alta + 2:
            excess = alta[ideal_alta + 1:]
            filtered = [n for n in filtered if n not in excess]

        # Filtro 3: Evitar muitos consecutivos
        sorted_filtered = sorted(filtered)
        consecutive_groups = []
        current_group = [sorted_filtered[0]] if sorted_filtered else []

        for i in range(1, len(sorted_filtered)):
            if sorted_filtered[i] - sorted_filtered[i-1] == 1:
                current_group.append(sorted_filtered[i])
            else:
                if len(current_group) > 1:
                    consecutive_groups.append(current_group)
                current_group = [sorted_filtered[i]]

        if len(current_group) > 1:
            consecutive_groups.append(current_group)

        # Remover alguns consecutivos se houver muitos
        for group in consecutive_groups:
            if len(group) > 3:  # Manter máximo 3 consecutivos
                to_remove = group[3:]
                filtered = [n for n in filtered if n not in to_remove]

        return filtered[:len(numbers)]  # Manter tamanho original

class UltimatePredictor:
    """Preditor definitivo combinando todas as técnicas"""

    def __init__(self):
        self.statistical_analyzer = AdvancedStatisticalAnalyzer()
        self.ensemble_predictor = EnsemblePredictor()
        self.filter_system = AdaptiveFilterSystem()
        self.is_trained = False

    def train(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Treinar todos os componentes"""
        print("Iniciando treinamento completo...")

        results = {}

        # 1. Análise estatística
        print("1. Executando análise estatística avançada...")
        self.statistical_analysis = self.statistical_analyzer.comprehensive_frequency_analysis(df)
        results['statistical_analysis'] = 'completed'

        # 2. Treinar ensemble
        print("2. Treinando ensemble de modelos...")
        try:
            ensemble_results = self.ensemble_predictor.train_with_cross_validation(df)
            results['ensemble'] = ensemble_results
        except Exception as e:
            print(f"Erro no ensemble: {e}")
            results['ensemble'] = None

        self.df = df
        self.is_trained = True

        return results

    def predict(self, n_predictions: int = 18) -> Dict[str, Any]:
        """Fazer predição final"""
        if not self.is_trained:
            raise ValueError("Preditor não foi treinado!")

        predictions = {}

        # 1. Predição estatística
        print("Gerando predição estatística...")
        stat_pred = self.statistical_analyzer.predict_by_statistical_model(self.df, n_predictions)
        predictions['statistical'] = stat_pred

        # 2. Predição por ensemble (se disponível)
        if self.ensemble_predictor.models:
            print("Gerando predição por ensemble...")
            try:
                ensemble_pred = self.ensemble_predictor.predict(self.df, n_predictions)
                predictions['ensemble'] = ensemble_pred
            except Exception as e:
                print(f"Erro na predição ensemble: {e}")
                predictions['ensemble'] = stat_pred  # Fallback
        else:
            predictions['ensemble'] = stat_pred

        # 3. Combinação inteligente
        print("Combinando predições...")
        combined_scores = Counter()

        # Votar com pesos
        weights = {'statistical': 0.4, 'ensemble': 0.6}

        for method, numbers in predictions.items():
            weight = weights.get(method, 0.5)
            for i, num in enumerate(numbers[:n_predictions]):
                # Peso decrescente por posição
                position_weight = (n_predictions - i) / n_predictions
                combined_scores[num] += weight * position_weight

        # Seleção final
        sorted_by_score = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        final_candidates = [num for num, _ in sorted_by_score[:n_predictions + 5]]

        # 4. Aplicar filtros adaptativos
        print("Aplicando filtros adaptativos...")
        final_prediction = self.filter_system.apply_statistical_filters(final_candidates, self.df)

        # Garantir quantidade correta
        if len(final_prediction) < n_predictions:
            remaining = [n for n in range(1, 26) if n not in final_prediction]
            np.random.shuffle(remaining)
            final_prediction.extend(remaining[:n_predictions - len(final_prediction)])

        final_prediction = sorted(final_prediction[:n_predictions])

        # 5. Análise de confiança
        confidence = self._calculate_confidence(predictions, final_prediction)

        return {
            'final_prediction': final_prediction,
            'individual_predictions': predictions,
            'confidence_score': confidence,
            'method_weights': weights
        }

    def _calculate_confidence(self, predictions: Dict[str, List[int]], final: List[int]) -> float:
        """Calcular score de confiança"""
        if len(predictions) < 2:
            return 0.5

        # Calcular consenso entre métodos
        all_numbers = []
        for nums in predictions.values():
            all_numbers.extend(nums)

        consensus_scores = Counter(all_numbers)

        # Score baseado no consenso dos números finais
        confidence = 0
        for num in final:
            consensus_count = consensus_scores.get(num, 0)
            confidence += consensus_count / len(predictions)

        return min(1.0, confidence / len(final))

    def validate_prediction(self, true_numbers: List[int], predicted_numbers: List[int]) -> Dict[str, Any]:
        """Validar predição"""
        true_set = set(true_numbers)
        pred_set = set(predicted_numbers)

        hits = len(true_set & pred_set)
        accuracy = hits / 15 * 100

        return {
            'hits': hits,
            'accuracy': accuracy,
            'hit_numbers': sorted(list(true_set & pred_set)),
            'missed_numbers': sorted(list(true_set - pred_set)),
            'false_positives': sorted(list(pred_set - true_set))
        }

def load_data_from_csv(file_path: str) -> pd.DataFrame:
    """Carregar dados do CSV"""
    try:
        df = pd.read_csv(file_path, sep=';')
        print(f"Dados carregados: {len(df)} concursos")
        return df
    except Exception as e:
        print(f"Erro ao carregar dados: {e}")
        return None

def main_execution():
    """Execução principal otimizada"""
    print("🎯 SISTEMA PREDITOR LOTOFÁCIL - VERSÃO OTIMIZADA")
    print("=" * 60)

    # Carregar dados
    df = load_data_from_csv("Lotof.csv")  # Substitua pelo caminho correto
    if df is None:
        print("Erro: Não foi possível carregar os dados")
        return

    # Inicializar preditor
    predictor = UltimatePredictor()

    # Treinar
    print("\n🔧 TREINANDO SISTEMA...")
    training_results = predictor.train(df)

    # Predizer
    print("\n🎯 GERANDO PREDIÇÕES...")
    prediction_result = predictor.predict(18)

    # Exibir resultados
    print("\n" + "="*60)
    print("📋 RESULTADO FINAL")
    print("="*60)

    final_numbers = prediction_result['final_prediction']
    confidence = prediction_result['confidence_score']

    print(f"\n🎯 NÚMEROS PREDITOS PARA PRÓXIMO CONCURSO:")
    print(f"   {final_numbers}")
    print(f"\n📊 CONFIANÇA: {confidence:.2%}")

    # Análise detalhada
    print(f"\n📈 ANÁLISE DETALHADA:")
    pares = sum(1 for n in final_numbers if n % 2 == 0)
    baixa = sum(1 for n in final_numbers if n <= 8)
    media = sum(1 for n in final_numbers if 9 <= n <= 17)
    alta = sum(1 for n in final_numbers if n >= 18)

    print(f"   • Números pares: {pares} ({pares/18*100:.1f}%)")
    print(f"   • Números ímpares: {18-pares} ({(18-pares)/18*100:.1f}%)")
    print(f"   • Faixa baixa (1-8): {baixa}")
    print(f"   • Faixa média (9-17): {media}")
    print(f"   • Faixa alta (18-25): {alta}")
    print(f"   • Soma total: {sum(final_numbers)}")

    # Predições individuais
    print(f"\n🔍 PREDIÇÕES POR MÉTODO:")
    for method, numbers in prediction_result['individual_predictions'].items():
        print(f"   {method.upper()}: {numbers}")

    return predictor, prediction_result

def test_historical_accuracy():
    """Testar acurácia com dados históricos"""
    print("🧪 TESTE DE ACURÁCIA HISTÓRICA")
    print("=" * 50)

    df = load_data_from_csv("Lotof.csv")
    if df is None:
        return

    # Usar 80% dos dados para treino, 20% para teste
    train_size = int(len(df) * 0.8)
    train_df = df.iloc[:train_size].copy()
    test_df = df.iloc[train_size:].copy()

    predictor = UltimatePredictor()
    predictor.train(train_df)

    results = []
    total_hits = 0

    print(f"\nTestando com {len(test_df)} concursos...")

    for i in range(len(test_df)):
        # Usar dados até o concurso i para predição
        current_train = pd.concat([train_df, test_df.iloc[:i]])

        # Re-treinar com dados atualizados (simulação de aprendizado online)
        if i % 10 == 0:  # Re-treinar a cada 10 concursos
            predictor.train(current_train)

        # Fazer predição
        try:
            prediction = predictor.predict(18)
            predicted_numbers = prediction['final_prediction']
        except:
            # Fallback para predição estatística simples se der erro
            predicted_numbers = list(range(1, 19))

        # Números reais do concurso
        true_numbers = [test_df.iloc[i][f'Bola{j}'] for j in range(1, 16)]

        # Validar
        validation = predictor.validate_prediction(true_numbers, predicted_numbers)

        results.append({
            'concurso': test_df.iloc[i]['Concurso'],
            'hits': validation['hits'],
            'accuracy': validation['accuracy'],
            'predicted': predicted_numbers,
            'actual': true_numbers
        })

        total_hits += validation['hits']

        if i < 5:  # Mostrar primeiros 5 resultados
            print(f"Concurso {test_df.iloc[i]['Concurso']}: {validation['hits']}/15 acertos")

    # Estatísticas finais
    avg_hits = total_hits / len(test_df)
    avg_accuracy = np.mean([r['accuracy'] for r in results])

    print(f"\n📊 RESULTADOS DO TESTE:")
    print(f"   • Total de concursos testados: {len(test_df)}")
    print(f"   • Acertos médios por concurso: {avg_hits:.2f}")
    print(f"   • Acurácia média: {avg_accuracy:.1f}%")
    print(f"   • Total de acertos: {total_hits}/{len(test_df) * 15}")

    # Distribuição de acertos
    hit_distribution = Counter([r['hits'] for r in results])
    print(f"\n📈 DISTRIBUIÇÃO DE ACERTOS:")
    for hits in sorted(hit_distribution.keys()):
        count = hit_distribution[hits]
        percentage = count / len(results) * 100
        print(f"   • {hits} acertos: {count} vezes ({percentage:.1f}%)")

    return results

class OptimizedPredictor:
    """Versão ainda mais otimizada com técnicas avançadas"""

    def __init__(self):
        self.models = {}
        self.meta_learner = None
        self.feature_importance = None

    def advanced_feature_selection(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Seleção de features mais sofisticada"""
        features = {}

        # 1. Features de momentum (tendência)
        for window in [5, 10, 20]:
            momentum_features = []
            for i in range(window, len(df)):
                recent_numbers = []
                for j in range(i-window, i):
                    row_numbers = [df.iloc[j][f'Bola{k}'] for k in range(1, 16)]
                    recent_numbers.extend(row_numbers)

                # Calcular momentum para cada número
                number_momentum = []
                for num in range(1, 26):
                    count = recent_numbers.count(num)
                    momentum = count / (window * 15)  # Normalizado
                    number_momentum.append(momentum)

                momentum_features.append(number_momentum)

            features[f'momentum_{window}'] = np.array(momentum_features)

        # 2. Features de volatilidade
        volatility_features = []
        for i in range(10, len(df)):
            recent_sums = []
            for j in range(i-10, i):
                row_numbers = [df.iloc[j][f'Bola{k}'] for k in range(1, 16)]
                recent_sums.append(sum(row_numbers))

            volatility = np.std(recent_sums)
            volatility_features.append(volatility)

        features['volatility'] = np.array(volatility_features).reshape(-1, 1)

        # 3. Features de correlação entre posições
        correlation_features = []
        for i in range(20, len(df)):
            correlations = []
            recent_data = df.iloc[i-20:i]

            for pos1 in range(1, 16):
                for pos2 in range(pos1+1, 16):
                    col1 = [recent_data.iloc[j][f'Bola{pos1}'] for j in range(len(recent_data))]
                    col2 = [recent_data.iloc[j][f'Bola{pos2}'] for j in range(len(recent_data))]

                    corr = np.corrcoef(col1, col2)[0, 1]
                    correlations.append(corr if not np.isnan(corr) else 0)

            correlation_features.append(correlations)

        features['correlations'] = np.array(correlation_features)

        return features

    def train_meta_model(self, df: pd.DataFrame):
        """Treinar meta-modelo que combina predições"""
        print("Treinando meta-modelo...")

        # Gerar features avançadas
        advanced_features = self.advanced_feature_selection(df)

        # Criar dataset para meta-aprendizado
        # (usando validação temporal)

        meta_X = []
        meta_y = []

        window_size = 50
        for i in range(window_size, len(df) - 1):
            # Features do momento atual
            current_features = []

            # Adicionar features de momentum
            if i >= len(advanced_features['momentum_5']):
                momentum_idx = len(advanced_features['momentum_5']) - 1
            else:
                momentum_idx = i - window_size + len(advanced_features['momentum_5']) - (len(df) - window_size)

            if momentum_idx >= 0 and momentum_idx < len(advanced_features['momentum_5']):
                current_features.extend(advanced_features['momentum_5'][momentum_idx])
            else:
                current_features.extend([0] * 25)  # Padding

            meta_X.append(current_features)

            # Target: próximo sorteio
            next_numbers = [df.iloc[i+1][f'Bola{j}'] for j in range(1, 16)]
            meta_y.append(next_numbers)

        if len(meta_X) > 0:
            meta_X = np.array(meta_X)
            meta_y = np.array(meta_y)

            # Treinar meta-modelo (um para cada posição)
            self.meta_learner = []
            for pos in range(15):
                model = xgb.XGBRegressor(
                    n_estimators=100,
                    max_depth=4,
                    learning_rate=0.1,
                    random_state=42
                )
                model.fit(meta_X, meta_y[:, pos])
                self.meta_learner.append(model)

    def predict_with_meta_model(self, df: pd.DataFrame) -> List[int]:
        """Predição usando meta-modelo"""
        if not self.meta_learner:
            return list(range(1, 19))  # Fallback

        # Gerar features para predição
        advanced_features = self.advanced_feature_selection(df)

        # Usar últimas features disponíveis
        if len(advanced_features['momentum_5']) > 0:
            last_features = advanced_features['momentum_5'][-1]
        else:
            last_features = [0] * 25

        # Predição
        predictions = []
        for pos in range(15):
            pred = self.meta_learner[pos].predict([last_features])[0]
            predictions.append(max(1, min(25, round(pred))))

        # Remover duplicatas e completar
        unique_predictions = []
        seen = set()
        for pred in predictions:
            if pred not in seen:
                unique_predictions.append(pred)
                seen.add(pred)

        # Completar se necessário
        for num in range(1, 26):
            if len(unique_predictions) >= 18:
                break
            if num not in seen:
                unique_predictions.append(num)

        return sorted(unique_predictions[:18])

# Função para executar versão completa otimizada
def run_optimized_system():
    """Executar sistema completamente otimizado"""
    print("🚀 SISTEMA PREDITOR LOTOFÁCIL - VERSÃO ULTRA OTIMIZADA")
    print("=" * 70)

    df = load_data_from_csv("Lotof.csv")
    if df is None:
        return

    # Treinar sistema básico
    basic_predictor = UltimatePredictor()
    basic_predictor.train(df)
    basic_result = basic_predictor.predict(18)

    # Treinar sistema avançado
    advanced_predictor = OptimizedPredictor()
    advanced_predictor.train_meta_model(df)
    advanced_result = advanced_predictor.predict_with_meta_model(df)

    # Combinar resultados
    combined_scores = Counter()

    # Votos do sistema básico (peso 0.6)
    for num in basic_result['final_prediction']:
        combined_scores[num] += 0.6

    # Votos do sistema avançado (peso 0.4)
    for num in advanced_result:
        combined_scores[num] += 0.4

    # Resultado final
    final_prediction = [num for num, _ in combined_scores.most_common(18)]

    print(f"\n🎯 PREDIÇÃO ULTRA OTIMIZADA:")
    print(f"   {sorted(final_prediction)}")

    print(f"\n🔍 COMPARAÇÃO DE MÉTODOS:")
    print(f"   Sistema Básico: {basic_result['final_prediction']}")
    print(f"   Sistema Avançado: {sorted(advanced_result)}")
    print(f"   Combinação Final: {sorted(final_prediction)}")

    # Confiança combinada
    basic_confidence = basic_result['confidence_score']
    combined_confidence = (basic_confidence + 0.7) / 2  # Meta-modelo tem confiança 0.7

    print(f"\n📊 CONFIANÇA FINAL: {combined_confidence:.2%}")

    return {
        'final_prediction': sorted(final_prediction),
        'basic_prediction': basic_result['final_prediction'],
        'advanced_prediction': sorted(advanced_result),
        'confidence': combined_confidence
    }

if __name__ == "__main__":
    print("Escolha o modo de execução:")
    print("1. Predição padrão")
    print("2. Teste de acurácia histórica")
    print("3. Sistema ultra otimizado")

    choice = input("Digite 1, 2 ou 3: ").strip()

    if choice == "1":
        main_execution()
    elif choice == "2":
        test_historical_accuracy()
    elif choice == "3":
        run_optimized_system()
    else:
        print("Opção inválida, executando modo padrão...")
        main_execution()