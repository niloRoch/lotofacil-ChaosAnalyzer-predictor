# -*- coding: utf-8 -*-
"""CHaos_otimizadoLF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12BIddeVlPpypCcOiAHbNwiOMZV6EjS4Z
"""

# -*- coding: utf-8 -*-
"""
PREDITOR LOTOF√ÅCIL APRIMORADO - VERS√ÉO 2.0
Melhorias focadas em assertividade:
- An√°lise estat√≠stica mais rigorosa
- Ensemble com valida√ß√£o cruzada
- Filtros adaptativos
- Otimiza√ß√£o de hiperpar√¢metros
"""

import pandas as pd
import numpy as np
import warnings
from collections import defaultdict, Counter
from typing import List, Tuple, Dict, Any, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import xgboost as xgb
from scipy import stats
from scipy.optimize import minimize
from scipy.signal import find_peaks, savgol_filter
import itertools
from tqdm import tqdm
from google import colab as cl
warnings.filterwarnings('ignore')

#importando arq
file_upload = cl.files.upload()

class AdvancedStatisticalAnalyzer:
    """Analisador estat√≠stico avan√ßado com foco em padr√µes reais"""

    def __init__(self):
        self.frequency_analysis = {}
        self.position_analysis = {}
        self.pattern_cache = {}

    def comprehensive_frequency_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """An√°lise de frequ√™ncia abrangente"""
        analysis = {}

        # Frequ√™ncia global de cada n√∫mero
        global_freq = Counter()
        position_freq = {i: Counter() for i in range(1, 16)}

        for _, row in df.iterrows():
            numbers = [row[f'Bola{i}'] for i in range(1, 16)]
            for pos, num in enumerate(numbers, 1):
                global_freq[num] += 1
                position_freq[pos][num] += 1

        analysis['global_frequency'] = dict(global_freq)
        analysis['position_frequency'] = {pos: dict(freq) for pos, freq in position_freq.items()}

        # An√°lise de tend√™ncias temporais (√∫ltimos N concursos)
        for window in [10, 20, 50, 100]:
            if len(df) >= window:
                recent_df = df.tail(window)
                recent_freq = Counter()
                for _, row in recent_df.iterrows():
                    numbers = [row[f'Bola{i}'] for i in range(1, 16)]
                    for num in numbers:
                        recent_freq[num] += 1

                analysis[f'frequency_last_{window}'] = dict(recent_freq)

        # An√°lise de periodicidade
        analysis['periodicity'] = self._analyze_periodicity(df)

        # An√°lise de gaps (intervalos entre apari√ß√µes)
        analysis['gap_analysis'] = self._analyze_gaps(df)

        return analysis

    def _analyze_periodicity(self, df: pd.DataFrame) -> Dict[int, Dict[str, float]]:
        """Analisar periodicidade de cada n√∫mero"""
        periodicity = {}

        for num in range(1, 26):
            appearances = []
            for i, row in df.iterrows():
                numbers = [row[f'Bola{j}'] for j in range(1, 16)]
                if num in numbers:
                    appearances.append(i)

            if len(appearances) > 2:
                intervals = np.diff(appearances)
                periodicity[num] = {
                    'mean_interval': np.mean(intervals),
                    'std_interval': np.std(intervals),
                    'median_interval': np.median(intervals),
                    'last_appearance': len(df) - 1 - appearances[-1] if appearances else len(df)
                }
            else:
                periodicity[num] = {
                    'mean_interval': len(df),
                    'std_interval': 0,
                    'median_interval': len(df),
                    'last_appearance': len(df)
                }

        return periodicity

    def _analyze_gaps(self, df: pd.DataFrame) -> Dict[int, List[int]]:
        """Analisar gaps entre apari√ß√µes"""
        gaps = {num: [] for num in range(1, 26)}
        last_seen = {num: -1 for num in range(1, 26)}

        for i, row in df.iterrows():
            numbers = [row[f'Bola{j}'] for j in range(1, 16)]

            # Calcular gaps para n√∫meros que apareceram
            for num in numbers:
                if last_seen[num] != -1:
                    gap = i - last_seen[num]
                    gaps[num].append(gap)
                last_seen[num] = i

        return gaps

    def predict_by_statistical_model(self, df: pd.DataFrame, n_predictions: int = 18) -> List[int]:
        """Predi√ß√£o baseada em modelo estat√≠stico avan√ßado"""
        analysis = self.comprehensive_frequency_analysis(df)

        # Calcular scores para cada n√∫mero
        scores = {}
        total_draws = len(df)

        for num in range(1, 26):
            score = 0

            # 1. Score baseado em frequ√™ncia global normalizada
            global_freq = analysis['global_frequency'].get(num, 0)
            expected_freq = total_draws * 15 / 25  # Frequ√™ncia esperada
            freq_score = (expected_freq - global_freq) / expected_freq  # Favorece n√∫meros "atrasados"

            # 2. Score baseado em tend√™ncia recente
            recent_trend = 0
            for window in [10, 20, 50]:
                if f'frequency_last_{window}' in analysis:
                    recent_freq = analysis[f'frequency_last_{window}'].get(num, 0)
                    expected_recent = window * 15 / 25
                    trend_component = (expected_recent - recent_freq) / expected_recent
                    recent_trend += trend_component * (1 / window)  # Peso maior para janelas menores

            # 3. Score baseado em periodicidade
            periodicity = analysis['periodicity'][num]
            last_gap = periodicity['last_appearance']
            mean_interval = periodicity['mean_interval']

            if mean_interval > 0:
                periodicity_score = min(2.0, last_gap / mean_interval)
            else:
                periodicity_score = 1.0

            # 4. Score baseado em distribui√ß√£o de gaps
            gaps = analysis['gap_analysis'][num]
            if gaps:
                gap_mean = np.mean(gaps)
                gap_std = np.std(gaps) if len(gaps) > 1 else gap_mean

                # Probabilidade baseada na distribui√ß√£o normal dos gaps
                if gap_std > 0:
                    z_score = (last_gap - gap_mean) / gap_std
                    gap_probability = stats.norm.cdf(z_score)
                else:
                    gap_probability = 0.5
            else:
                gap_probability = 1.0

            # Combinar scores com pesos otimizados
            score = (
                freq_score * 0.3 +
                recent_trend * 0.25 +
                periodicity_score * 0.25 +
                gap_probability * 0.2
            )

            scores[num] = score

        # Selecionar n√∫meros com maiores scores
        sorted_numbers = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers[:n_predictions]]

class SmartFeatureEngineering:
    """Engenharia de features inteligente"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_selector = None

    def create_advanced_features(self, df: pd.DataFrame) -> np.ndarray:
        """Criar features avan√ßadas para ML"""
        features = []

        # Para cada concurso, criar features
        for i in range(len(df)):
            row_features = []

            # 1. Features b√°sicas do sorteio atual
            numbers = [df.iloc[i][f'Bola{j}'] for j in range(1, 16)]

            # Estat√≠sticas descritivas
            row_features.extend([
                np.mean(numbers),
                np.std(numbers),
                np.median(numbers),
                np.min(numbers),
                np.max(numbers),
                np.sum(numbers)
            ])

            # 2. Features de distribui√ß√£o
            # N√∫meros por faixa
            baixa = sum(1 for n in numbers if n <= 8)
            media = sum(1 for n in numbers if 9 <= n <= 17)
            alta = sum(1 for n in numbers if n >= 18)

            row_features.extend([baixa, media, alta])

            # Paridade
            pares = sum(1 for n in numbers if n % 2 == 0)
            impares = 15 - pares
            row_features.extend([pares, impares])

            # 3. Features temporais (lookback)
            lookback_features = []
            for lookback in [1, 2, 3, 5, 10]:
                if i >= lookback:
                    # N√∫meros dos sorteios anteriores
                    prev_numbers = []
                    for j in range(max(0, i-lookback), i):
                        prev_nums = [df.iloc[j][f'Bola{k}'] for k in range(1, 16)]
                        prev_numbers.extend(prev_nums)

                    if prev_numbers:
                        lookback_features.extend([
                            np.mean(prev_numbers),
                            np.std(prev_numbers),
                            len(set(prev_numbers))  # N√∫meros √∫nicos
                        ])
                    else:
                        lookback_features.extend([0, 0, 0])
                else:
                    lookback_features.extend([0, 0, 0])

            row_features.extend(lookback_features)

            # 4. Features de padr√µes
            # Sequ√™ncias consecutivas
            sorted_numbers = sorted(numbers)
            consecutive_count = 0
            for j in range(len(sorted_numbers) - 1):
                if sorted_numbers[j+1] - sorted_numbers[j] == 1:
                    consecutive_count += 1
            row_features.append(consecutive_count)

            # Gaps entre n√∫meros
            gaps = [sorted_numbers[j+1] - sorted_numbers[j] for j in range(len(sorted_numbers) - 1)]
            if gaps:
                row_features.extend([np.mean(gaps), np.std(gaps)])
            else:
                row_features.extend([0, 0])

            # 5. Features de frequ√™ncia hist√≥rica
            if i > 0:
                # Frequ√™ncia dos n√∫meros no hist√≥rico at√© este ponto
                historical_df = df.iloc[:i]
                freq_features = []

                for num in numbers:
                    count = 0
                    for _, hist_row in historical_df.iterrows():
                        hist_numbers = [hist_row[f'Bola{k}'] for k in range(1, 16)]
                        if num in hist_numbers:
                            count += 1

                    freq_features.append(count / len(historical_df) if len(historical_df) > 0 else 0)

                # Estat√≠sticas das frequ√™ncias
                if freq_features:
                    row_features.extend([
                        np.mean(freq_features),
                        np.std(freq_features),
                        np.min(freq_features),
                        np.max(freq_features)
                    ])
                else:
                    row_features.extend([0, 0, 0, 0])
            else:
                row_features.extend([0, 0, 0, 0])

            features.append(row_features)

        return np.array(features)

    def select_best_features(self, X: np.ndarray, y: np.ndarray, k: int = 50) -> np.ndarray:
        """Selecionar as melhores features"""
        if self.feature_selector is None:
            self.feature_selector = SelectKBest(score_func=mutual_info_regression, k=min(k, X.shape[1]))
            X_selected = self.feature_selector.fit_transform(X, y.flatten())
        else:
            X_selected = self.feature_selector.transform(X)

        return X_selected

class EnsemblePredictor:
    """Ensemble de m√∫ltiplos algoritmos otimizado"""

    def __init__(self):
        self.models = {}
        self.weights = {}
        self.scaler = RobustScaler()
        self.feature_engineer = SmartFeatureEngineering()

    def create_models(self) -> Dict[str, Any]:
        """Criar ensemble de modelos otimizados"""
        models = {}

        # Random Forest otimizado
        models['rf'] = RandomForestRegressor(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )

        # XGBoost otimizado
        models['xgb'] = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )

        # Gradient Boosting
        models['gb'] = GradientBoostingRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42
        )

        # Neural Network
        models['mlp'] = MLPRegressor(
            hidden_layer_sizes=(100, 50, 25),
            activation='relu',
            solver='adam',
            alpha=0.001,
            learning_rate='adaptive',
            max_iter=500,
            random_state=42
        )

        return models

    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Preparar dados para treinamento"""
        # Criar features
        X = self.feature_engineer.create_advanced_features(df)

        # Criar targets (pr√≥ximo sorteio)
        y = []
        for i in range(len(df) - 1):
            next_numbers = [df.iloc[i+1][f'Bola{j}'] for j in range(1, 16)]
            y.append(next_numbers)

        # Ajustar X para corresponder a y
        X = X[:-1]  # Remover √∫ltima linha que n√£o tem target
        y = np.array(y)

        # Selecionar melhores features
        X_selected = self.feature_engineer.select_best_features(X, y)

        # Normalizar
        X_normalized = self.scaler.fit_transform(X_selected)

        return X_normalized, y

    def train_with_cross_validation(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Treinar com valida√ß√£o cruzada temporal"""
        X, y = self.prepare_training_data(df)

        if len(X) < 10:
            raise ValueError("Dados insuficientes para treinamento")

        models = self.create_models()
        results = {}

        # Valida√ß√£o cruzada temporal
        tscv = TimeSeriesSplit(n_splits=5)

        print("Treinando ensemble com valida√ß√£o cruzada...")

        for name, model in tqdm(models.items()):
            scores = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                # Treinar modelo para cada posi√ß√£o
                position_scores = []
                for pos in range(15):
                    model_copy = type(model)(**model.get_params())
                    model_copy.fit(X_train, y_train[:, pos])

                    y_pred = model_copy.predict(X_val)
                    score = -mean_absolute_error(y_val[:, pos], y_pred)
                    position_scores.append(score)

                scores.append(np.mean(position_scores))

            avg_score = np.mean(scores)
            results[name] = {
                'model': model,
                'cv_score': avg_score,
                'cv_std': np.std(scores)
            }

            print(f"{name}: CV Score = {avg_score:.4f} (+/- {np.std(scores):.4f})")

        # Treinar modelos finais
        self.models = {}
        for name, result in results.items():
            model = result['model']
            # Treinar um modelo para cada posi√ß√£o
            position_models = []
            for pos in range(15):
                model_copy = type(model)(**model.get_params())
                model_copy.fit(X, y[:, pos])
                position_models.append(model_copy)
            self.models[name] = position_models

        # Calcular pesos baseados na performance
        scores = [results[name]['cv_score'] for name in results.keys()]
        min_score = min(scores)
        shifted_scores = [score - min_score + 1e-6 for score in scores]
        total_score = sum(shifted_scores)

        self.weights = {
            name: shifted_scores[i] / total_score
            for i, name in enumerate(results.keys())
        }

        print(f"\nPesos finais: {self.weights}")

        return results

    def predict(self, df: pd.DataFrame, n_predictions: int = 18) -> List[int]:
        """Fazer predi√ß√£o usando ensemble"""
        if not self.models:
            raise ValueError("Modelos n√£o foram treinados")

        # Preparar features para predi√ß√£o
        X = self.feature_engineer.create_advanced_features(df)
        X_selected = self.feature_engineer.feature_selector.transform(X[-1:])
        X_normalized = self.scaler.transform(X_selected)

        # Predi√ß√µes de cada modelo
        ensemble_predictions = []

        for name, position_models in self.models.items():
            model_pred = []
            for pos in range(15):
                pred = position_models[pos].predict(X_normalized)[0]
                model_pred.append(max(1, min(25, round(pred))))

            ensemble_predictions.append(model_pred)

        # Combinar predi√ß√µes com pesos
        final_scores = {num: 0 for num in range(1, 26)}

        for i, predictions in enumerate(ensemble_predictions):
            model_name = list(self.models.keys())[i]
            weight = self.weights[model_name]

            for pred in predictions:
                final_scores[pred] += weight

        # Adicionar ru√≠do pequeno para quebrar empates
        for num in final_scores:
            final_scores[num] += np.random.normal(0, 0.01)

        # Selecionar top n√∫meros
        sorted_numbers = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers[:n_predictions]]

class AdaptiveFilterSystem:
    """Sistema de filtros adaptativos"""

    def __init__(self):
        self.filters = []

    def apply_statistical_filters(self, numbers: List[int], df: pd.DataFrame) -> List[int]:
        """Aplicar filtros estat√≠sticos"""
        filtered = numbers.copy()

        # Filtro 1: Balanceamento par/√≠mpar
        pares = [n for n in filtered if n % 2 == 0]
        impares = [n for n in filtered if n % 2 == 1]

        # An√°lise hist√≥rica de paridade
        historical_ratios = []
        for _, row in df.tail(50).iterrows():
            row_numbers = [row[f'Bola{i}'] for i in range(1, 16)]
            pares_hist = sum(1 for n in row_numbers if n % 2 == 0)
            historical_ratios.append(pares_hist / 15)

        target_par_ratio = np.mean(historical_ratios)
        target_pares = int(len(filtered) * target_par_ratio)

        # Ajustar para atingir ratio hist√≥rico
        if len(pares) > target_pares:
            # Remover alguns pares
            excess_pares = pares[target_pares:]
            filtered = [n for n in filtered if n not in excess_pares]
        elif len(pares) < target_pares:
            # Adicionar pares se poss√≠vel
            needed_pares = target_pares - len(pares)
            available_pares = [n for n in range(2, 26, 2) if n not in filtered]
            if available_pares:
                to_add = available_pares[:needed_pares]
                filtered.extend(to_add)

        # Filtro 2: Distribui√ß√£o por faixas
        baixa = [n for n in filtered if n <= 8]
        media = [n for n in filtered if 9 <= n <= 17]
        alta = [n for n in filtered if n >= 18]

        # Balancear faixas baseado no hist√≥rico
        ideal_baixa = len(filtered) // 3
        ideal_media = len(filtered) // 3
        ideal_alta = len(filtered) - ideal_baixa - ideal_media

        # Ajustes simples
        if len(baixa) > ideal_baixa + 2:
            excess = baixa[ideal_baixa + 1:]
            filtered = [n for n in filtered if n not in excess]

        if len(alta) > ideal_alta + 2:
            excess = alta[ideal_alta + 1:]
            filtered = [n for n in filtered if n not in excess]

        # Filtro 3: Evitar muitos consecutivos
        sorted_filtered = sorted(filtered)
        consecutive_groups = []
        current_group = [sorted_filtered[0]] if sorted_filtered else []

        for i in range(1, len(sorted_filtered)):
            if sorted_filtered[i] - sorted_filtered[i-1] == 1:
                current_group.append(sorted_filtered[i])
            else:
                if len(current_group) > 1:
                    consecutive_groups.append(current_group)
                current_group = [sorted_filtered[i]]

        if len(current_group) > 1:
            consecutive_groups.append(current_group)

        # Remover alguns consecutivos se houver muitos
        for group in consecutive_groups:
            if len(group) > 3:  # Manter m√°ximo 3 consecutivos
                to_remove = group[3:]
                filtered = [n for n in filtered if n not in to_remove]

        return filtered[:len(numbers)]  # Manter tamanho original

class UltimatePredictor:
    """Preditor definitivo combinando todas as t√©cnicas"""

    def __init__(self):
        self.statistical_analyzer = AdvancedStatisticalAnalyzer()
        self.ensemble_predictor = EnsemblePredictor()
        self.filter_system = AdaptiveFilterSystem()
        self.is_trained = False

    def train(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Treinar todos os componentes"""
        print("Iniciando treinamento completo...")

        results = {}

        # 1. An√°lise estat√≠stica
        print("1. Executando an√°lise estat√≠stica avan√ßada...")
        self.statistical_analysis = self.statistical_analyzer.comprehensive_frequency_analysis(df)
        results['statistical_analysis'] = 'completed'

        # 2. Treinar ensemble
        print("2. Treinando ensemble de modelos...")
        try:
            ensemble_results = self.ensemble_predictor.train_with_cross_validation(df)
            results['ensemble'] = ensemble_results
        except Exception as e:
            print(f"Erro no ensemble: {e}")
            results['ensemble'] = None

        self.df = df
        self.is_trained = True

        return results

    def predict(self, n_predictions: int = 18) -> Dict[str, Any]:
        """Fazer predi√ß√£o final"""
        if not self.is_trained:
            raise ValueError("Preditor n√£o foi treinado!")

        predictions = {}

        # 1. Predi√ß√£o estat√≠stica
        print("Gerando predi√ß√£o estat√≠stica...")
        stat_pred = self.statistical_analyzer.predict_by_statistical_model(self.df, n_predictions)
        predictions['statistical'] = stat_pred

        # 2. Predi√ß√£o por ensemble (se dispon√≠vel)
        if self.ensemble_predictor.models:
            print("Gerando predi√ß√£o por ensemble...")
            try:
                ensemble_pred = self.ensemble_predictor.predict(self.df, n_predictions)
                predictions['ensemble'] = ensemble_pred
            except Exception as e:
                print(f"Erro na predi√ß√£o ensemble: {e}")
                predictions['ensemble'] = stat_pred  # Fallback
        else:
            predictions['ensemble'] = stat_pred

        # 3. Combina√ß√£o inteligente
        print("Combinando predi√ß√µes...")
        combined_scores = Counter()

        # Votar com pesos
        weights = {'statistical': 0.4, 'ensemble': 0.6}

        for method, numbers in predictions.items():
            weight = weights.get(method, 0.5)
            for i, num in enumerate(numbers[:n_predictions]):
                # Peso decrescente por posi√ß√£o
                position_weight = (n_predictions - i) / n_predictions
                combined_scores[num] += weight * position_weight

        # Sele√ß√£o final
        sorted_by_score = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        final_candidates = [num for num, _ in sorted_by_score[:n_predictions + 5]]

        # 4. Aplicar filtros adaptativos
        print("Aplicando filtros adaptativos...")
        final_prediction = self.filter_system.apply_statistical_filters(final_candidates, self.df)

        # Garantir quantidade correta
        if len(final_prediction) < n_predictions:
            remaining = [n for n in range(1, 26) if n not in final_prediction]
            np.random.shuffle(remaining)
            final_prediction.extend(remaining[:n_predictions - len(final_prediction)])

        final_prediction = sorted(final_prediction[:n_predictions])

        # 5. An√°lise de confian√ßa
        confidence = self._calculate_confidence(predictions, final_prediction)

        return {
            'final_prediction': final_prediction,
            'individual_predictions': predictions,
            'confidence_score': confidence,
            'method_weights': weights
        }

    def _calculate_confidence(self, predictions: Dict[str, List[int]], final: List[int]) -> float:
        """Calcular score de confian√ßa"""
        if len(predictions) < 2:
            return 0.5

        # Calcular consenso entre m√©todos
        all_numbers = []
        for nums in predictions.values():
            all_numbers.extend(nums)

        consensus_scores = Counter(all_numbers)

        # Score baseado no consenso dos n√∫meros finais
        confidence = 0
        for num in final:
            consensus_count = consensus_scores.get(num, 0)
            confidence += consensus_count / len(predictions)

        return min(1.0, confidence / len(final))

    def validate_prediction(self, true_numbers: List[int], predicted_numbers: List[int]) -> Dict[str, Any]:
        """Validar predi√ß√£o"""
        true_set = set(true_numbers)
        pred_set = set(predicted_numbers)

        hits = len(true_set & pred_set)
        accuracy = hits / 15 * 100

        return {
            'hits': hits,
            'accuracy': accuracy,
            'hit_numbers': sorted(list(true_set & pred_set)),
            'missed_numbers': sorted(list(true_set - pred_set)),
            'false_positives': sorted(list(pred_set - true_set))
        }

def load_data_from_csv(file_path: str) -> pd.DataFrame:
    """Carregar dados do CSV"""
    try:
        df = pd.read_csv(file_path, sep=';')
        print(f"Dados carregados: {len(df)} concursos")
        return df
    except Exception as e:
        print(f"Erro ao carregar dados: {e}")
        return None

def main_execution():
    """Execu√ß√£o principal otimizada"""
    print("üéØ SISTEMA PREDITOR LOTOF√ÅCIL - VERS√ÉO OTIMIZADA")
    print("=" * 60)

    # Carregar dados
    df = load_data_from_csv("Lotof.csv")  # Substitua pelo caminho correto
    if df is None:
        print("Erro: N√£o foi poss√≠vel carregar os dados")
        return

    # Inicializar preditor
    predictor = UltimatePredictor()

    # Treinar
    print("\nüîß TREINANDO SISTEMA...")
    training_results = predictor.train(df)

    # Predizer
    print("\nüéØ GERANDO PREDI√á√ïES...")
    prediction_result = predictor.predict(18)

    # Exibir resultados
    print("\n" + "="*60)
    print("üìã RESULTADO FINAL")
    print("="*60)

    final_numbers = prediction_result['final_prediction']
    confidence = prediction_result['confidence_score']

    print(f"\nüéØ N√öMEROS PREDITOS PARA PR√ìXIMO CONCURSO:")
    print(f"   {final_numbers}")
    print(f"\nüìä CONFIAN√áA: {confidence:.2%}")

    # An√°lise detalhada
    print(f"\nüìà AN√ÅLISE DETALHADA:")
    pares = sum(1 for n in final_numbers if n % 2 == 0)
    baixa = sum(1 for n in final_numbers if n <= 8)
    media = sum(1 for n in final_numbers if 9 <= n <= 17)
    alta = sum(1 for n in final_numbers if n >= 18)

    print(f"   ‚Ä¢ N√∫meros pares: {pares} ({pares/18*100:.1f}%)")
    print(f"   ‚Ä¢ N√∫meros √≠mpares: {18-pares} ({(18-pares)/18*100:.1f}%)")
    print(f"   ‚Ä¢ Faixa baixa (1-8): {baixa}")
    print(f"   ‚Ä¢ Faixa m√©dia (9-17): {media}")
    print(f"   ‚Ä¢ Faixa alta (18-25): {alta}")
    print(f"   ‚Ä¢ Soma total: {sum(final_numbers)}")

    # Predi√ß√µes individuais
    print(f"\nüîç PREDI√á√ïES POR M√âTODO:")
    for method, numbers in prediction_result['individual_predictions'].items():
        print(f"   {method.upper()}: {numbers}")

    return predictor, prediction_result

def test_historical_accuracy():
    """Testar acur√°cia com dados hist√≥ricos"""
    print("üß™ TESTE DE ACUR√ÅCIA HIST√ìRICA")
    print("=" * 50)

    df = load_data_from_csv("Lotof.csv")
    if df is None:
        return

    # Usar 80% dos dados para treino, 20% para teste
    train_size = int(len(df) * 0.8)
    train_df = df.iloc[:train_size].copy()
    test_df = df.iloc[train_size:].copy()

    predictor = UltimatePredictor()
    predictor.train(train_df)

    results = []
    total_hits = 0

    print(f"\nTestando com {len(test_df)} concursos...")

    for i in range(len(test_df)):
        # Usar dados at√© o concurso i para predi√ß√£o
        current_train = pd.concat([train_df, test_df.iloc[:i]])

        # Re-treinar com dados atualizados (simula√ß√£o de aprendizado online)
        if i % 10 == 0:  # Re-treinar a cada 10 concursos
            predictor.train(current_train)

        # Fazer predi√ß√£o
        try:
            prediction = predictor.predict(18)
            predicted_numbers = prediction['final_prediction']
        except:
            # Fallback para predi√ß√£o estat√≠stica simples se der erro
            predicted_numbers = list(range(1, 19))

        # N√∫meros reais do concurso
        true_numbers = [test_df.iloc[i][f'Bola{j}'] for j in range(1, 16)]

        # Validar
        validation = predictor.validate_prediction(true_numbers, predicted_numbers)

        results.append({
            'concurso': test_df.iloc[i]['Concurso'],
            'hits': validation['hits'],
            'accuracy': validation['accuracy'],
            'predicted': predicted_numbers,
            'actual': true_numbers
        })

        total_hits += validation['hits']

        if i < 5:  # Mostrar primeiros 5 resultados
            print(f"Concurso {test_df.iloc[i]['Concurso']}: {validation['hits']}/15 acertos")

    # Estat√≠sticas finais
    avg_hits = total_hits / len(test_df)
    avg_accuracy = np.mean([r['accuracy'] for r in results])

    print(f"\nüìä RESULTADOS DO TESTE:")
    print(f"   ‚Ä¢ Total de concursos testados: {len(test_df)}")
    print(f"   ‚Ä¢ Acertos m√©dios por concurso: {avg_hits:.2f}")
    print(f"   ‚Ä¢ Acur√°cia m√©dia: {avg_accuracy:.1f}%")
    print(f"   ‚Ä¢ Total de acertos: {total_hits}/{len(test_df) * 15}")

    # Distribui√ß√£o de acertos
    hit_distribution = Counter([r['hits'] for r in results])
    print(f"\nüìà DISTRIBUI√á√ÉO DE ACERTOS:")
    for hits in sorted(hit_distribution.keys()):
        count = hit_distribution[hits]
        percentage = count / len(results) * 100
        print(f"   ‚Ä¢ {hits} acertos: {count} vezes ({percentage:.1f}%)")

    return results

class OptimizedPredictor:
    """Vers√£o ainda mais otimizada com t√©cnicas avan√ßadas"""

    def __init__(self):
        self.models = {}
        self.meta_learner = None
        self.feature_importance = None

    def advanced_feature_selection(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Sele√ß√£o de features mais sofisticada"""
        features = {}

        # 1. Features de momentum (tend√™ncia)
        for window in [5, 10, 20]:
            momentum_features = []
            for i in range(window, len(df)):
                recent_numbers = []
                for j in range(i-window, i):
                    row_numbers = [df.iloc[j][f'Bola{k}'] for k in range(1, 16)]
                    recent_numbers.extend(row_numbers)

                # Calcular momentum para cada n√∫mero
                number_momentum = []
                for num in range(1, 26):
                    count = recent_numbers.count(num)
                    momentum = count / (window * 15)  # Normalizado
                    number_momentum.append(momentum)

                momentum_features.append(number_momentum)

            features[f'momentum_{window}'] = np.array(momentum_features)

        # 2. Features de volatilidade
        volatility_features = []
        for i in range(10, len(df)):
            recent_sums = []
            for j in range(i-10, i):
                row_numbers = [df.iloc[j][f'Bola{k}'] for k in range(1, 16)]
                recent_sums.append(sum(row_numbers))

            volatility = np.std(recent_sums)
            volatility_features.append(volatility)

        features['volatility'] = np.array(volatility_features).reshape(-1, 1)

        # 3. Features de correla√ß√£o entre posi√ß√µes
        correlation_features = []
        for i in range(20, len(df)):
            correlations = []
            recent_data = df.iloc[i-20:i]

            for pos1 in range(1, 16):
                for pos2 in range(pos1+1, 16):
                    col1 = [recent_data.iloc[j][f'Bola{pos1}'] for j in range(len(recent_data))]
                    col2 = [recent_data.iloc[j][f'Bola{pos2}'] for j in range(len(recent_data))]

                    corr = np.corrcoef(col1, col2)[0, 1]
                    correlations.append(corr if not np.isnan(corr) else 0)

            correlation_features.append(correlations)

        features['correlations'] = np.array(correlation_features)

        return features

    def train_meta_model(self, df: pd.DataFrame):
        """Treinar meta-modelo que combina predi√ß√µes"""
        print("Treinando meta-modelo...")

        # Gerar features avan√ßadas
        advanced_features = self.advanced_feature_selection(df)

        # Criar dataset para meta-aprendizado
        # (usando valida√ß√£o temporal)

        meta_X = []
        meta_y = []

        window_size = 50
        for i in range(window_size, len(df) - 1):
            # Features do momento atual
            current_features = []

            # Adicionar features de momentum
            if i >= len(advanced_features['momentum_5']):
                momentum_idx = len(advanced_features['momentum_5']) - 1
            else:
                momentum_idx = i - window_size + len(advanced_features['momentum_5']) - (len(df) - window_size)

            if momentum_idx >= 0 and momentum_idx < len(advanced_features['momentum_5']):
                current_features.extend(advanced_features['momentum_5'][momentum_idx])
            else:
                current_features.extend([0] * 25)  # Padding

            meta_X.append(current_features)

            # Target: pr√≥ximo sorteio
            next_numbers = [df.iloc[i+1][f'Bola{j}'] for j in range(1, 16)]
            meta_y.append(next_numbers)

        if len(meta_X) > 0:
            meta_X = np.array(meta_X)
            meta_y = np.array(meta_y)

            # Treinar meta-modelo (um para cada posi√ß√£o)
            self.meta_learner = []
            for pos in range(15):
                model = xgb.XGBRegressor(
                    n_estimators=100,
                    max_depth=4,
                    learning_rate=0.1,
                    random_state=42
                )
                model.fit(meta_X, meta_y[:, pos])
                self.meta_learner.append(model)

    def predict_with_meta_model(self, df: pd.DataFrame) -> List[int]:
        """Predi√ß√£o usando meta-modelo"""
        if not self.meta_learner:
            return list(range(1, 19))  # Fallback

        # Gerar features para predi√ß√£o
        advanced_features = self.advanced_feature_selection(df)

        # Usar √∫ltimas features dispon√≠veis
        if len(advanced_features['momentum_5']) > 0:
            last_features = advanced_features['momentum_5'][-1]
        else:
            last_features = [0] * 25

        # Predi√ß√£o
        predictions = []
        for pos in range(15):
            pred = self.meta_learner[pos].predict([last_features])[0]
            predictions.append(max(1, min(25, round(pred))))

        # Remover duplicatas e completar
        unique_predictions = []
        seen = set()
        for pred in predictions:
            if pred not in seen:
                unique_predictions.append(pred)
                seen.add(pred)

        # Completar se necess√°rio
        for num in range(1, 26):
            if len(unique_predictions) >= 18:
                break
            if num not in seen:
                unique_predictions.append(num)

        return sorted(unique_predictions[:18])

# Fun√ß√£o para executar vers√£o completa otimizada
def run_optimized_system():
    """Executar sistema completamente otimizado"""
    print("üöÄ SISTEMA PREDITOR LOTOF√ÅCIL - VERS√ÉO ULTRA OTIMIZADA")
    print("=" * 70)

    df = load_data_from_csv("Lotof.csv")
    if df is None:
        return

    # Treinar sistema b√°sico
    basic_predictor = UltimatePredictor()
    basic_predictor.train(df)
    basic_result = basic_predictor.predict(18)

    # Treinar sistema avan√ßado
    advanced_predictor = OptimizedPredictor()
    advanced_predictor.train_meta_model(df)
    advanced_result = advanced_predictor.predict_with_meta_model(df)

    # Combinar resultados
    combined_scores = Counter()

    # Votos do sistema b√°sico (peso 0.6)
    for num in basic_result['final_prediction']:
        combined_scores[num] += 0.6

    # Votos do sistema avan√ßado (peso 0.4)
    for num in advanced_result:
        combined_scores[num] += 0.4

    # Resultado final
    final_prediction = [num for num, _ in combined_scores.most_common(18)]

    print(f"\nüéØ PREDI√á√ÉO ULTRA OTIMIZADA:")
    print(f"   {sorted(final_prediction)}")

    print(f"\nüîç COMPARA√á√ÉO DE M√âTODOS:")
    print(f"   Sistema B√°sico: {basic_result['final_prediction']}")
    print(f"   Sistema Avan√ßado: {sorted(advanced_result)}")
    print(f"   Combina√ß√£o Final: {sorted(final_prediction)}")

    # Confian√ßa combinada
    basic_confidence = basic_result['confidence_score']
    combined_confidence = (basic_confidence + 0.7) / 2  # Meta-modelo tem confian√ßa 0.7

    print(f"\nüìä CONFIAN√áA FINAL: {combined_confidence:.2%}")

    return {
        'final_prediction': sorted(final_prediction),
        'basic_prediction': basic_result['final_prediction'],
        'advanced_prediction': sorted(advanced_result),
        'confidence': combined_confidence
    }

if __name__ == "__main__":
    print("Escolha o modo de execu√ß√£o:")
    print("1. Predi√ß√£o padr√£o")
    print("2. Teste de acur√°cia hist√≥rica")
    print("3. Sistema ultra otimizado")

    choice = input("Digite 1, 2 ou 3: ").strip()

    if choice == "1":
        main_execution()
    elif choice == "2":
        test_historical_accuracy()
    elif choice == "3":
        run_optimized_system()
    else:
        print("Op√ß√£o inv√°lida, executando modo padr√£o...")
        main_execution()